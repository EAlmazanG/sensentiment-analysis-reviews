{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed and cleaned data\n",
    "processed_data_path = '../data/processed/'\n",
    "raw_data_path = '../data/raw/'\n",
    "\n",
    "name = 'hd'\n",
    "\n",
    "reviews_pro = pd.read_csv(processed_data_path + name + '_reviews.csv')\n",
    "resumme_raw = pd.read_csv(raw_data_path + 'resumme_' + name + '.csv')\n",
    "\n",
    "display(resumme_raw)\n",
    "display(reviews_pro.sample(5))\n",
    "\n",
    "reviews = reviews_pro.copy()\n",
    "resumme = resumme_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First draft summary plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average for each score\n",
    "average_food = reviews['food_score'].mean()\n",
    "average_service = reviews['service_score'].mean()\n",
    "average_atmosphere = reviews['atmosphere_score'].mean()\n",
    "average_reviews = (resumme_raw['stars'] * resumme_raw['reviews']).sum() / resumme_raw['reviews'].sum()\n",
    "\n",
    "# Create a figure with horizontal subplots\n",
    "fig = make_subplots(rows=1, cols=3, \n",
    "                    specs=[[{\"type\": \"xy\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]], \n",
    "                    subplot_titles=(\"Average Score\", \"Number of Reviews\", \"Categories\"))\n",
    "\n",
    "# First subplot: Display the average review as large text\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0], y=[0], text=[f\"{average_reviews:.2f}\"], mode=\"text\", textfont=dict(size=120)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\n",
    "fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\n",
    "\n",
    "\n",
    "# Second subplot: Bar plot for reviews\n",
    "fig.add_trace(\n",
    "    go.Bar(x=resumme_raw['reviews'], y=resumme_raw['stars'], marker=dict(color='lightskyblue'),\n",
    "           text=resumme_raw['reviews'], textposition='auto', name=\"Reviews\", orientation='h'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Third subplot: Bar plot for categories (Food, Service, Atmosphere)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=[average_food, average_service, average_atmosphere], \n",
    "           y=['Food', 'Service', 'Atmosphere'], \n",
    "           marker=dict(color='lightgreen'), \n",
    "           text=[f\"{average_food:.2f}\", f\"{average_service:.2f}\", f\"{average_atmosphere:.2f}\"], \n",
    "           textposition='auto', \n",
    "           orientation='h', \n",
    "           name=\"Categories\"),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, width=1200,  plot_bgcolor=\"white\", paper_bgcolor=\"white\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime format\n",
    "reviews['date'] = pd.to_datetime(reviews['date'], errors='coerce')\n",
    "reviews['month'] = reviews['date'].dt.to_period('M')\n",
    "reviews['year'] = reviews['date'].dt.year\n",
    "reviews['week'] = reviews['date'].dt.to_period('W')\n",
    "reviews['week'] = reviews['date'] - pd.to_timedelta(reviews['date'].dt.weekday, unit='d')\n",
    "reviews['week'] = reviews['week'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Filter data for the last periods (months, years, weeks)\n",
    "last_months = reviews[reviews['date'] >= pd.to_datetime('today') - pd.DateOffset(months=12)]\n",
    "last_years = reviews[reviews['date'] >= pd.to_datetime('today') - pd.DateOffset(years=8)]\n",
    "last_weeks = reviews[reviews['date'] >= pd.to_datetime('today') - pd.DateOffset(weeks=5)]\n",
    "\n",
    "# Compute averages for the required periods\n",
    "monthly_avg_scores = last_months.groupby('month')[['rating_score', 'food_score', 'service_score', 'atmosphere_score']].mean()\n",
    "yearly_avg_scores = last_years.groupby('year')[['rating_score']].mean()\n",
    "weekly_avg_scores = last_weeks.groupby('week')[['rating_score', 'food_score', 'service_score', 'atmosphere_score']].mean()\n",
    "\n",
    "# Update the axis labels for each score to be more readable\n",
    "label_mapping = {\n",
    "    'rating_score': 'Rating',\n",
    "    'food_score': 'Food',\n",
    "    'service_score': 'Service',\n",
    "    'atmosphere_score': 'Atmosphere'\n",
    "}\n",
    "\n",
    "# Create a figure with subplots using the Z-layout\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    specs=[[{\"colspan\": 2}, None],\n",
    "                           [{}, {}]],  # 1 large plot on the first row, 2 smaller plots on the second\n",
    "                    subplot_titles=(\"Monthly Score Trends (Last 12 Months)\", \n",
    "                                    \"Annual Rating Score Trends (Last 6 Years)\", \n",
    "                                    \"Weekly Score Trends (Last 4 Weeks)\"))\n",
    "\n",
    "# Add monthly score trends to the first row (rating_score in stronger color)\n",
    "colors = ['#1f77b4', '#aec7e8', '#aec7e8', '#aec7e8']\n",
    "for i, column in enumerate(monthly_avg_scores.columns):\n",
    "    label = label_mapping[column]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=monthly_avg_scores.index.astype(str), y=monthly_avg_scores[column],\n",
    "                   mode='lines+markers', name=label, \n",
    "                   text=[f\"{label} - {val:.2f}\" for val in monthly_avg_scores[column]], \n",
    "                   hoverinfo=\"text\", line=dict(color=colors[i])),\n",
    "        row=1, col=1)\n",
    "\n",
    "# Add yearly score trends to the second row (left)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=yearly_avg_scores.index.astype(str), y=yearly_avg_scores['rating_score'],\n",
    "               mode='lines+markers', name=\"Rating\", line=dict(color='#1f77b4', width=4),\n",
    "               text=[f\"Rating - {val:.2f}\" for val in yearly_avg_scores['rating_score']], \n",
    "               hoverinfo=\"text\"),\n",
    "    row=2, col=1)\n",
    "\n",
    "# Add weekly score trends to the second row (right, weaker colors)\n",
    "for i, column in enumerate(weekly_avg_scores.columns):\n",
    "    label = label_mapping[column]  # Get the readable label\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=weekly_avg_scores.index.astype(str), y=weekly_avg_scores[column],\n",
    "                   mode='lines+markers', name=label, \n",
    "                   text=[f\"{label} - {val:.2f}\" for val in weekly_avg_scores[column]], \n",
    "                   hoverinfo=\"text\", line=dict(color=colors[i])),\n",
    "        row=2, col=2)\n",
    "\n",
    "# Enhance presentation: remove gridlines and borders, increase size, and remove legend\n",
    "fig.update_layout(showlegend=False, \n",
    "                  title=\"Score Trends Analysis\",\n",
    "                  title_font=dict(size=28),\n",
    "                  margin=dict(l=50, r=50, t=100, b=50),\n",
    "                  paper_bgcolor=\"white\",\n",
    "                  height=800, width=1200)\n",
    "fig.update_xaxes(showline=False, showgrid=False)\n",
    "fig.update_yaxes(showline=False, showgrid=True)\n",
    "\n",
    "# Customize x-axes formatting: show only the year for yearly data, and only day and month for weekly data\n",
    "fig.update_xaxes(\n",
    "    tickformat=\"%Y\",  # Only show the year for the yearly graph\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickformat=\"%d-%b\",  # Show only the day and month for weekly graph\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add annotations to highlight key points\n",
    "fig.add_annotation(x='2024-06', y=4.8, \n",
    "                   text=\"Highest Score\", \n",
    "                   showarrow=True, arrowhead=2,\n",
    "                   ax=0, ay=80, row=1, col=1, font=dict(size=14))\n",
    "\n",
    "fig.add_annotation(x='2024-03', y=4.5, \n",
    "                   text=\"Drop in March\", \n",
    "                   showarrow=True, arrowhead=2,\n",
    "                   ax=0, ay=-40, row=1, col=1, font=dict(size=14))\n",
    "\n",
    "fig.add_annotation(x='2024-08', y=4.5, \n",
    "                   text=\"Drop in August\", \n",
    "                   showarrow=True, arrowhead=2,\n",
    "                   ax=0, ay=-40, row=1, col=1, font=dict(size=14))\n",
    "\n",
    "fig.update_traces(marker=dict(size=8), selector=dict(name=\"Rating\"))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK stopwords and lexicon\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load spaCy Spanish model\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text, stopworks and tokenize words\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-záéíóúñü0-9\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token.lemma_ for token in doc \n",
    "              if token.text not in stop_words and not token.is_punct and not token.is_space]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Cleaning Reviews\")\n",
    "reviews['cleaned_review'] = reviews['review'].fillna('').progress_apply(clean_text)\n",
    "\n",
    "display(reviews[['review', 'cleaned_review']].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and Sentiment\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Word Clouds and Visualization\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentiment for each review using \n",
    "def analyzeSentiment(df):\n",
    "    # Initialize VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Apply sentiment analysis to each review using VADER\n",
    "    df['vader_sentiment'] = df['cleaned_review'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    \n",
    "    # Classify sentiment into positive, neutral, negative using rating_score and vader_sentiment\n",
    "    def classify_sentiment(row):\n",
    "        if row['rating_score'] >= 4:\n",
    "            return 'positive'\n",
    "        elif row['rating_score'] <= 2:\n",
    "            return 'negative'\n",
    "        elif row['vader_sentiment'] > 0.05:\n",
    "            return 'positive'\n",
    "        elif row['vader_sentiment'] < -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    df['sentiment_label'] = df.apply(classify_sentiment, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract most common words for a selected sentiment\n",
    "def extractCommonWords(df, sentiment_label='positive', n=10):\n",
    "    # Filter reviews by sentiment label\n",
    "    filtered_reviews = df[df['sentiment_label'] == sentiment_label]['cleaned_review'].fillna('').tolist()\n",
    "    \n",
    "    # Tokenize and count words for the given sentiment label\n",
    "    vectorizer = CountVectorizer().fit(filtered_reviews)\n",
    "    word_counts = vectorizer.transform(filtered_reviews).sum(axis=0)\n",
    "    \n",
    "    # Create a dictionary of word frequencies\n",
    "    word_freq = [(word, word_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    return sorted_word_freq\n",
    "\n",
    "# Extract most common n-grams for a selected sentiment\n",
    "def extractCommonNgrams(df, sentiment_label='positive', n=2, top_n=10):\n",
    "    # Filter reviews by sentiment label\n",
    "    filtered_reviews = df[df['sentiment_label'] == sentiment_label]['cleaned_review'].fillna('').tolist()\n",
    "    \n",
    "    # Create n-grams for the given sentiment label\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n)).fit(filtered_reviews)\n",
    "    ngram_counts = vectorizer.transform(filtered_reviews).sum(axis=0)\n",
    "    \n",
    "    # Create a list of n-grams with their counts\n",
    "    ngram_freq = [(word, ngram_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    sorted_ngrams = sorted(ngram_freq, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return sorted_ngrams\n",
    "\n",
    "# Analyze sentiment with VADER\n",
    "reviews = analyzeSentiment(reviews)\n",
    "\n",
    "# Extract common positive and negative phrases\n",
    "common_positive_words = extractCommonWords(reviews, sentiment_label = 'positive', n = 10)\n",
    "common_negative_words = extractCommonWords(reviews, sentiment_label = 'negative', n = 10)\n",
    "\n",
    "print(\"Top Positive Words:\", common_positive_words)\n",
    "print(\"Top Negative Words:\", common_negative_words)\n",
    "\n",
    "# Extract common positive and negative bigrams\n",
    "common_positive_bigrams = extractCommonNgrams(reviews, sentiment_label='positive', n=2, top_n=10)\n",
    "common_negative_bigrams = extractCommonNgrams(reviews, sentiment_label='negative', n=2, top_n=10)\n",
    "\n",
    "print(\"Top Positive Bigrams:\", common_positive_bigrams)\n",
    "print(\"Top Negative Bigrams:\", common_negative_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of distribution of reviews on time based on sentiments\n",
    "def plotSentimentTrend(df, years_limit = 6):\n",
    "    # Convert date to datetime format and handle missing values\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "    \n",
    "    # Filter only the last 6 years\n",
    "    last_six_years = datetime.datetime.now() - pd.DateOffset(years=years_limit)\n",
    "    df = df[df['date'] >= last_six_years]\n",
    "\n",
    "    # Set date as index for resampling\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Resample to monthly and count sentiments\n",
    "    sentiment_counts = df.resample('M')['sentiment_label'].value_counts().unstack().fillna(0)\n",
    "\n",
    "    # Calculate the percentage for each sentiment type\n",
    "    sentiment_percentage = sentiment_counts.div(sentiment_counts.sum(axis=1), axis=0) * 100\n",
    "    sentiment_percentage = sentiment_percentage.round(2)\n",
    "    sentiment_percentage = sentiment_percentage.reset_index().melt(id_vars=['date'], value_name='percentage', var_name='sentiment_label')\n",
    "    \n",
    "    # Plot sentiment percentage evolution\n",
    "    fig = px.area(\n",
    "        sentiment_percentage,\n",
    "        x='date',\n",
    "        y='percentage',\n",
    "        color='sentiment_label',\n",
    "        title='Sentiment Percentage Over the Last 6 Years',\n",
    "        labels={'date': '', 'percentage': 'Percentage of Reviews (%)', 'sentiment_label': 'Sentiment'},\n",
    "        template='plotly_white',\n",
    "    )\n",
    "\n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title=dict(x=0.5, xanchor='center', font=dict(size=18, color='black')),\n",
    "        xaxis=dict(showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=True, title='Percentage of Reviews', ticksuffix='%'),\n",
    "        legend=dict(title='', orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "        margin=dict(l=20, r=20, t=50, b=20),\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        hovermode='x unified',\n",
    "        width=1200,\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    # Customize color for sentiment categories\n",
    "    color_map = {\n",
    "        'positive': 'rgba(102, 194, 165, 0.7)', \n",
    "        'neutral': 'rgba(141, 160, 203, 0.7)', \n",
    "        'negative': 'rgba(252, 141, 98, 0.7)'\n",
    "    }\n",
    "    fig.for_each_trace(lambda trace: trace.update(line=dict(width=0, shape='spline'), fill='tonexty', fillcolor=color_map.get(trace.name, 'rgba(150, 150, 150, 0.5)')))\n",
    "\n",
    "    # Remove the plot frame and keep the visualization as clean as possible\n",
    "    fig.update_xaxes(showline=False)\n",
    "    fig.update_yaxes(showline=False, range=[0, 100])  # Percentage scale from 0 to 100\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plotSentimentTrend(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract most and least recommendations mentioned\n",
    "def analyzeRecommendations(df):\n",
    "    all_dishes = []\n",
    "\n",
    "    # Convert string representation of lists to actual lists and extend all_dishes\n",
    "    for item in df['recommendations_list'].dropna():\n",
    "        try:\n",
    "            dishes = ast.literal_eval(item)\n",
    "            if isinstance(dishes, list):\n",
    "                all_dishes.extend(dishes)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Filter out empty values\n",
    "    all_dishes = [dish for dish in all_dishes if dish.strip() != '']\n",
    "\n",
    "    # Count the frequency of each dish\n",
    "    dish_counts = Counter(all_dishes)\n",
    "\n",
    "    # Most and least recommended dishes\n",
    "    most_common_dishes = dish_counts.most_common(3)\n",
    "    min_count = min(dish_counts.values())\n",
    "    worst_dishes = [dish for dish, count in dish_counts.items() if count == min_count]\n",
    "\n",
    "    print(\"Top Most Recommended:\", most_common_dishes)\n",
    "    print(\"Least Recommended :\", worst_dishes)\n",
    "\n",
    "analyzeRecommendations(reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings for each cleaned review\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Import Bert model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "tqdm.pandas(desc=\"Generating Embeddings\")\n",
    "reviews['embedding'] = reviews['cleaned_review'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA Embeddings Visualization\n",
    "def visualizeEmbeddingsPCA(df):\n",
    "    # Convert embeddings to a NumPy array\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    ratings = df['rating_score']\n",
    "    \n",
    "    # Perform PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Calculate variance explained by each component\n",
    "    var_explained = pca.explained_variance_ratio_ * 100\n",
    "    var1, var2 = var_explained\n",
    "    \n",
    "    # Prepare DataFrame for Plotly\n",
    "    plot_df = pd.DataFrame({\n",
    "        'PCA Component 1': reduced_embeddings[:, 0],\n",
    "        'PCA Component 2': reduced_embeddings[:, 1],\n",
    "        'Rating Score': ratings,\n",
    "        'Review ID': df.get('review_id', range(len(df)))  # Optional identifier\n",
    "    })\n",
    "    \n",
    "    # Create interactive scatter plot\n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x='PCA Component 1',\n",
    "        y='PCA Component 2',\n",
    "        color='Rating Score',\n",
    "        color_continuous_scale='Viridis',\n",
    "        hover_data=['Review ID', 'Rating Score'],\n",
    "        title=f'Embeddings by Rating Score (PCA 1: {var1:.1f}%, PCA 2: {var2:.1f}%)',\n",
    "        labels={\n",
    "            'PCA Component 1': f'PCA 1 ({var1:.1f}% variance)',\n",
    "            'PCA Component 2': f'PCA 2 ({var2:.1f}% variance)',\n",
    "            'Rating Score': 'Rating Score'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Enhance layout for clarity\n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        coloraxis_colorbar=dict(\n",
    "            title='Rating Score',\n",
    "            tickmode='linear'\n",
    "        ),\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return reduced_embeddings\n",
    "\n",
    "embeddings_pca = visualizeEmbeddingsPCA(reviews)\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# UMAP Embeddings Visualization\n",
    "def visualizeEmbeddingsUMAP(df):\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    sentiment_labels = df['sentiment_label']\n",
    "\n",
    "    # Reduce dimensionality with UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    viz_df = pd.DataFrame(reduced_embeddings, columns=['x', 'y'])\n",
    "    viz_df['sentiment_label'] = sentiment_labels\n",
    "\n",
    "    # Scatter plot with Plotly for interactive visualization\n",
    "    fig = px.scatter(\n",
    "        viz_df,\n",
    "        x='x',\n",
    "        y='y',\n",
    "        color='sentiment_label',\n",
    "        title='Embedding Visualization with UMAP',\n",
    "        labels={'x': 'UMAP Dimension 1', 'y': 'UMAP Dimension 2'},\n",
    "        color_discrete_map={'positive': 'green', 'neutral': 'gray', 'negative': 'red'},\n",
    "        opacity=0.7\n",
    "    )\n",
    "    fig.update_layout(showlegend=True, legend=dict(title='Sentiment'), margin=dict(l=10, r=10, t=40, b=10))\n",
    "    fig.show()\n",
    "\n",
    "    return reduced_embeddings\n",
    "\n",
    "embeddings_umap = visualizeEmbeddingsUMAP(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Plot K distace for dbscan eps adjustment\n",
    "def plotKdistance(reduced_embeddings, k=5, method='PCA'):\n",
    "    # Compute k-nearest neighbors\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(reduced_embeddings)\n",
    "    distances, _ = neighbors_fit.kneighbors(reduced_embeddings)\n",
    "    \n",
    "    # Sort distances to the k-th nearest neighbor\n",
    "    k_distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "    # Create interactive line plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.arange(1, len(k_distances) + 1),\n",
    "        y=k_distances,\n",
    "        mode='lines',\n",
    "        line=dict(color='blue'),\n",
    "        name='k-distance'\n",
    "    ))\n",
    "    \n",
    "    # Update layout for clarity\n",
    "    fig.update_layout(\n",
    "        title=f'k-Distance Graph for {method}',\n",
    "        xaxis_title='Points sorted by distance',\n",
    "        yaxis_title=f'Distance to {k}th Nearest Neighbor',\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Add light grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "plotKdistance(embeddings_umap, k= 10, method='PCA')\n",
    "plotKdistance(embeddings_pca, k= 10, method='UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to apply DBSCAN\n",
    "def apply_dbscan(reduced_embeddings, eps=0.6, min_samples=5):\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(reduced_embeddings)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(scaled)\n",
    "    return labels\n",
    "\n",
    "# PCA Visualization with DBSCAN\n",
    "def visualizeEmbeddingsPCA_with_DBSCAN(df, eps=0.55, min_samples=10):\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    ratings = df['rating_score']\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(embeddings)\n",
    "    var1, var2 = pca.explained_variance_ratio_ * 100\n",
    "    \n",
    "    clusters = apply_dbscan(reduced, eps, min_samples)\n",
    "    \n",
    "    plot_df = pd.DataFrame({\n",
    "        'pca_component_1': reduced[:, 0],\n",
    "        'pca_component_2': reduced[:, 1],\n",
    "        'rating_score': ratings,\n",
    "        'pca_cluster': clusters,\n",
    "        'review_id': df.get('review_id', range(len(df)))\n",
    "    })\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x='pca_component_1',\n",
    "        y='pca_component_2',\n",
    "        color='pca_cluster',\n",
    "        color_continuous_scale='Viridis',\n",
    "        hover_data=['review_id', 'rating_score'],\n",
    "        title=f'PCA with DBSCAN (PCA1: {var1:.1f}%, PCA2: {var2:.1f}%)',\n",
    "        labels={\n",
    "            'PCA 1': f'pca_component_1 ({var1:.1f}% variance)',\n",
    "            'PCA 2': f'pca_component_2 ({var2:.1f}% variance)',\n",
    "            'Cluster': 'pca_cluster'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        coloraxis_colorbar=dict(title='pca_cluster'),\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return plot_df\n",
    "\n",
    "# UMAP Visualization with DBSCAN\n",
    "def visualizeEmbeddingsUMAP_with_DBSCAN(df, eps=0.7, min_samples=10):\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    sentiment = df['sentiment_label']\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    clusters = apply_dbscan(reduced, eps, min_samples)\n",
    "    \n",
    "    plot_df = pd.DataFrame({\n",
    "        'umap_component_1': reduced[:, 0],\n",
    "        'umap_component_2': reduced[:, 1],\n",
    "        'sentiment': sentiment,\n",
    "        'umap_cluster': clusters,\n",
    "        'review_id': df.get('review_id', range(len(df)))\n",
    "    })\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x='umap_component_1',\n",
    "        y='umap_component_2',\n",
    "        color='umap_cluster',\n",
    "        color_continuous_scale='Viridis',\n",
    "        hover_data=['sentiment', 'umap_cluster'],\n",
    "        title='UMAP with DBSCAN',\n",
    "        labels={\n",
    "            'UMAP 1': 'umap_component_1',\n",
    "            'UMAP 2': 'umap_component_2',\n",
    "            'Cluster': 'umap_cluster'\n",
    "        },\n",
    "        opacity=0.7\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        legend=dict(title='umap_cluster'),\n",
    "        margin=dict(l=10, r=10, t=40, b=10)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return plot_df\n",
    "\n",
    "# Visualize with DBSCAN clusters\n",
    "pca_clusters = visualizeEmbeddingsPCA_with_DBSCAN(reviews, eps=0.5, min_samples=5)\n",
    "umap_clusters = visualizeEmbeddingsUMAP_with_DBSCAN(reviews, eps=0.5, min_samples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load embeddings from reviews\n",
    "ebm_reviews = np.array(reviews['embedding'].tolist())\n",
    "\n",
    "# Calculate cosine similarity matrix between all pairs of embeddings\n",
    "similarity_matrix = cosine_similarity(ebm_reviews)\n",
    "\n",
    "# Set similarity threshold to create a sparser graph\n",
    "similarity_threshold = 0.75\n",
    "\n",
    "# Create a new sparser graph\n",
    "G_sparser = nx.Graph()\n",
    "\n",
    "# Add nodes representing each review\n",
    "for i in range(len(reviews)):\n",
    "    G_sparser.add_node(i, sentiment_label=reviews['sentiment_label'].iloc[i])\n",
    "\n",
    "# Add edges based on the similarity matrix and new threshold\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i + 1, len(similarity_matrix)):  # Only consider upper triangle to avoid redundancy\n",
    "        if similarity_matrix[i][j] >= similarity_threshold:\n",
    "            G_sparser.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "# Use Girvan-Newman algorithm to detect communities\n",
    "comp = nx.algorithms.community.girvan_newman(G_sparser)\n",
    "communities_sparser = tuple(sorted(c) for c in next(comp))\n",
    "\n",
    "# Extract key terms from each community using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3, stop_words='english')\n",
    "community_keywords = []\n",
    "\n",
    "for community in communities_sparser:\n",
    "    reviews_text = reviews.iloc[list(community)]['cleaned_review'].astype(str).tolist()\n",
    "    # Ensure there are non-stopword terms to avoid empty vocabulary error\n",
    "    filtered_reviews_text = [text for text in reviews_text if len(vectorizer.build_tokenizer()(text)) > 0]\n",
    "    if len(filtered_reviews_text) > 1:\n",
    "        tfidf_matrix = vectorizer.fit_transform(filtered_reviews_text)\n",
    "        keywords = vectorizer.get_feature_names_out()\n",
    "        community_keywords.append(\", \".join(keywords))\n",
    "    else:\n",
    "        community_keywords.append(reviews.iloc[list(community)[0]]['cleaned_review'])\n",
    "\n",
    "# Prepare data for Plotly interactive visualization\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_color = []\n",
    "node_text = []\n",
    "\n",
    "pos = nx.spring_layout(G_sparser, seed=42)\n",
    "colors = px.colors.qualitative.Set1  # A set of distinct colors for different communities\n",
    "\n",
    "# Extract node positions, colors, and labels for Plotly\n",
    "for i, community in enumerate(communities_sparser):\n",
    "    for node in community:\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_color.append(colors[i % len(colors)])\n",
    "        node_text.append(f\"{community_keywords[i]}\")\n",
    "\n",
    "# Create edge traces\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "\n",
    "for edge in G_sparser.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "# Create the Plotly figure\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='gray'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        line_width=2,\n",
    "        color=node_color\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title='Reviews by Communities',\n",
    "                    titlefont_size=16,\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20, l=5, r=5, t=40),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join PCA and UMAP clusters info to reviews\n",
    "reviews = reviews.merge(pca_clusters[['review_id','pca_cluster']]).merge(umap_clusters[['review_id','umap_cluster']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Extract topics using LDA model\n",
    "def analyzeTopicsLDA(df, number_of_topics = 5):\n",
    "   # Prepare corpus for LDA\n",
    "    cleaned_reviews = df['cleaned_review'].dropna().tolist()\n",
    "    tokenized_reviews = [review.split() for review in cleaned_reviews if isinstance(review, str) and review.strip() != '']\n",
    "    \n",
    "    if not tokenized_reviews:\n",
    "        print(\"No valid reviews to process.\")\n",
    "        return None, []\n",
    "    \n",
    "    dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "    if len(dictionary) == 0:\n",
    "        print(\"Dictionary is empty after tokenization.\")\n",
    "        return None, []\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(review) for review in tokenized_reviews]\n",
    "    if not any(corpus):\n",
    "        print(\"Corpus is empty. No terms found in any document.\")\n",
    "        return None, []\n",
    "    \n",
    "    # Train LDA model\n",
    "    try:\n",
    "        lda_model = LdaModel(\n",
    "            corpus,\n",
    "            num_topics=number_of_topics,\n",
    "            id2word=dictionary,\n",
    "            passes=10,\n",
    "            random_state=42\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"LDA Model training failed: {e}\")\n",
    "        return None, []\n",
    "    \n",
    "    # Extract topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(f\"Topic {topic[0]}: {topic[1]}\")\n",
    "    return lda_model, topics\n",
    "\n",
    "print('=== General topics ===')\n",
    "lda_model, topics = analyzeTopicsLDA(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['pca_cluster', 'umap_cluster', 'sentiment_label']\n",
    "\n",
    "# Initialize dictionary to store topics\n",
    "topics_dict = {group_col: {} for group_col in group_columns}\n",
    "\n",
    "# Iterate over each grouping column and generate topics\n",
    "for group_col in group_columns:\n",
    "    print(f\"\\n=== Topics by {group_col} ===\")\n",
    "    unique_groups = reviews[group_col].dropna().unique()\n",
    "    \n",
    "    for group_val in unique_groups:\n",
    "        subset = reviews[reviews[group_col] == group_val]\n",
    "        \n",
    "        # Check if there are enough reviews to train LDA\n",
    "        if len(subset) < 5:\n",
    "            print(f\"\\n--- {group_col} = {group_val} ---\")\n",
    "            print(\"Not enough data to train LDA.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- {group_col} = {group_val} ---\")\n",
    "        \n",
    "        # Generate topics for the current subset\n",
    "        lda_model, topics = analyzeTopicsLDA(subset)\n",
    "        \n",
    "        if lda_model is not None and topics:\n",
    "            # Store topics as strings in the dictionary\n",
    "            topics_strings = [topic[1] for topic in topics]\n",
    "            topics_dict[group_col][group_val] = topics_strings\n",
    "        else:\n",
    "            print(\"No topics generated for this group.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract moments with worst rating and process that reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyze low scores and extract negative reviews for the selected time period\n",
    "def analyzeLowScores(df, score_column, time_period='month', num_periods=1):\n",
    "    # Group by selected period and calculate average score\n",
    "    avg_scores = df.groupby(time_period)[score_column].mean().sort_values()\n",
    "    # Select the specified number of periods with the lowest average score\n",
    "    low_score_periods = avg_scores.index[:num_periods]\n",
    "    \n",
    "    # Filter negative reviews for the selected periods with the lowest score\n",
    "    period_reviews = df[(df[time_period].isin(low_score_periods)) & (df[score_column] < 3) & (df['sentiment_label'] == 'negative')]\n",
    "    # Drop the 'embedding' column if it exists to avoid issues with non-hashable types\n",
    "    if 'embedding' in period_reviews.columns:\n",
    "        period_reviews = period_reviews.drop(columns=['embedding'])\n",
    "    \n",
    "    # Add a column indicating the period with the lowest score for easier filtering\n",
    "    period_reviews['low_score_period'] = period_reviews[time_period]\n",
    "    period_reviews = period_reviews.sort_values('low_score_period')\n",
    "    return period_reviews\n",
    "\n",
    "# Usage\n",
    "time_period = 'month'  # Change to 'week', 'year', etc. to analyze different periods\n",
    "num_periods = 3  # Number of periods with the lowest average score to select\n",
    "\n",
    "# Analyze for each score type\n",
    "negative_periods_rating_reviews = analyzeLowScores(reviews, 'rating_score', time_period, num_periods)\n",
    "negative_periods_food_reviews = analyzeLowScores(reviews, 'food_score', time_period, num_periods)\n",
    "negative_periods_service_reviews = analyzeLowScores(reviews, 'service_score', time_period, num_periods)\n",
    "negative_periods_atmosphere_reviews = analyzeLowScores(reviews, 'atmosphere_score', time_period, num_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate topics for each low_score_period and concatenate results\n",
    "def generateTopicsPerPeriod(df, score_column, number_of_topics=1):\n",
    "    topics_dict = {score_column: {}}\n",
    "    for period in df['low_score_period'].unique():\n",
    "        period_reviews = df[df['low_score_period'] == period]\n",
    "        # Assuming analyzeTopicsLDA function returns topics as the second output\n",
    "        _, topics = analyzeTopicsLDA(period_reviews, number_of_topics=number_of_topics)\n",
    "        topics_dict[score_column][period] = topics\n",
    "    return topics_dict\n",
    "\n",
    "negative_periods_rating_topics = generateTopicsPerPeriod(negative_periods_rating_reviews, 'rating_score')\n",
    "negative_periods_food_topics = generateTopicsPerPeriod(negative_periods_food_reviews, 'food_score')\n",
    "negative_periods_service_topics = generateTopicsPerPeriod(negative_periods_service_reviews, 'service_score')\n",
    "negative_periods_atmosphere_topics = generateTopicsPerPeriod(negative_periods_atmosphere_reviews, 'atmosphere_score')\n",
    "\n",
    "negative_periods_topics = {**negative_periods_rating_topics, **negative_periods_food_topics, **negative_periods_service_topics, **negative_periods_atmosphere_topics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract outliers and pain points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worst 3 words\n",
    "# worst 3 bigrams\n",
    "\n",
    "# use topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together each one in a string and send it to gpt api to extract the main pain points to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_prompt = \"Using the provided LDA topics for different aggrupations (UMAP clustering, PCA clustering and sentiment Clustering, negative periods by one category score...), generate a list of key strengths and areas for improvement for the venue. The output should be clear, direct, and suitable for stakeholders, avoiding ambiguity. Ensure coherence between positive and negative points, with no contradictions. Organize into 'Key Strengths' and 'Areas for Improvement' with concise, complete ideas. LDA Topics: \"\n",
    "suggested_prompt_periods = \"Using the provided LDA topics for different aggrupations (UMAP clustering, PCA clustering and sentiment Clustering, negative periods by one category score...), generate a list of key strengths and areas for improvement for the venue. The output should be clear, direct, and suitable for stakeholders, avoiding ambiguity. Ensure coherence between positive and negative points, with no contradictions. Organize into 'Key Strengths' and 'Areas for Improvement' with concise, complete ideas. The info must be order and classify by the category of the score and the period of time. LDA Topics: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_prompt + str(negative_periods_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
